class Tokenizer:
    """ ğŸš€ Byte-level Latent Tokenizer / ë°”ì´íŠ¸ ë‹¨ìœ„ ì ì¬ í† í¬ë‚˜ì´ì €
    Handles UTF-8 bytes directly to support English, Korean, and Code.
    """
    def __init__(self, text=None):
        # We use a fixed 256 byte vocab + special tokens / 256ê°œ ë°”ì´íŠ¸ + íŠ¹ìˆ˜ í† í° ê³ ì • ì–´íœ˜ ì‚¬ì „
        self.vocab_size = 256 + 1 # +1 for <|endoftext|>
        self.eot_token = 256

    def encode(self, s):
        """ String to list of byte integers with EOT support / EOT ì§€ì›ì„ í¬í•¨í•œ ë¬¸ìì—´-ë°”ì´íŠ¸ ì •ìˆ˜ ë³€í™˜ """
        res = []
        # Split by <|endoftext|> marker / ë§ˆì»¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
        parts = s.split("<|endoftext|>")
        for i, part in enumerate(parts):
            res.extend(list(part.encode('utf-8')))
            if i < len(parts) - 1:
                res.append(self.eot_token)
        return res

    def decode(self, l):
        """ List of byte integers to string / ë°”ì´íŠ¸ ì •ìˆ˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ """
        # Filter out EOT for clean decoding / ê¹¨ë—í•œ ë””ì½”ë”©ì„ ìœ„í•´ EOT í•„í„°ë§
        bytes_list = bytes([b for b in l if b < 256])
        return bytes_list.decode('utf-8', errors='replace')

if __name__ == "__main__":
    # Test / í…ŒìŠ¤íŠ¸
    t = Tokenizer()
    test_str = "Hello, slmaker! ì•ˆë…•í•˜ì„¸ìš”! def code(): pass"
    encoded = t.encode(test_str)
    decoded = t.decode(encoded)
    print(f"Original: {test_str}")
    print(f"Encoded: {encoded[:10]}...")
    print(f"Decoded: {decoded}")
    assert test_str == decoded
    print("Tokenizer Upgrade Verified. / í† í¬ë‚˜ì´ì € ê³ ë„í™” ê²€ì¦ ì™„ë£Œ.")
